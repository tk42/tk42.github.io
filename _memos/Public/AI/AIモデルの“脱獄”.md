---
title: AIモデルの“脱獄”
date: 2023-12-15
tags: 
publish: true
feed: show
---

[GPT-4を含むAIモデルの“脱獄”、新手法が明らかに。研究者が安全対策強化を訴える \| WIRED.jp](https://wired.jp/article/automated-ai-attack-gpt-4/)

新たに見つかった脱獄の手法はこうだ。APIにリクエストが送信されると、別のAIシステムが脱獄を試みるプロンプトを生成して判断する。この手法はLLMの基本的な弱点を浮き彫りにしている。モデルを保護する既存の方法が十分でないことを示す[攻撃](https://wired.jp/article/openai-custom-chatbots-gpts-prompt-injection-attacks/)のうちのひとつで、最近判明したものだ。

一部のモデルは特定の攻撃を防ぐ対策をしている。しかし、そもそもこうしたモデルが機能する方法には固有の脆弱性があり、そのことが防御を難しくしていると、コルターは指摘する。「こうした脆弱性の多くは大規模言語モデルに固有なものであることを理解する必要があります」とコルターは話す。「そしてそれを防ぐための明確で確立された方法はないのです」

訓練データから学んだ偏見をもっており、プロンプトへの回答が簡潔ではない場合には情報を捏造する傾向がある。安全対策がないと、薬物の入手方法や爆弾のつくり方といった情報をユーザーに提供してしまうのだ。モデルの開発元はそうならないよう、モデルの回答が一貫性をもち、正確に見えるようにするための手法を活用している。これには人間がモデルの回答を評価し、そのフィードバックを基にモデルを微調整して、するべきではない回答をする可能性を低くするという工程が含まれている。

Robust Intelligenceが明らかにした手法は、人間による調整作業が攻撃に対してモデルを保護する確実な方法ではないことを示していると、ニューヨーク大学の助教授でコンピュータのセキュリティと機械学習を研究する[ブレンダン・ドラン=ギャヴィット](https://engineering.nyu.edu/faculty/brendan-dolan-gavitt)は話す。

GPT-4のようなLLMを基盤としたシステムを構築している企業は、追加の安全対策を施すべきだとドラン=ギャヴィットは指摘する。「LLMを使用するシステムを設計する際には、悪意のあるユーザーが脱獄して、許可のない情報にアクセスできないようにする必要があります」と語る。
